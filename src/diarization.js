import * as ort from 'onnxruntime-web';
import Meyda from 'meyda';
import * as mlKMeans from 'ml-kmeans';
const kmeans = mlKMeans.kmeans || mlKMeans.default || mlKMeans;

const wasmModuleUrl = new URL(
  '../node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.mjs',
  import.meta.url
).href;
const wasmBinaryUrl = new URL(
  '../node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm',
  import.meta.url
).href;

class BrowserSpeakerDiarization {
  constructor() {
    this.vadSession = null;
    this.embeddingSession = null;
    this.audioContext = null;
    this.vadStateMeta = null;
    this.vadState = null;
    this.vadSampleRateTensor = null;
    this.vadOutputName = null;
    this.vadStateOutputName = null;
    this.fixedNumSpeakers = 2; // è©±è€…æ•°ã‚’å›ºå®šï¼ˆ2äººï¼‰
    this.minOutputSegmentSec = 0.3; // ã“ã‚Œæœªæº€ã®çŸ­ã„åŒºé–“ã¯çµæœã¨ã—ã¦å‡ºåŠ›ã—ãªã„
  }
  
  async initialize() {
    // Configure ONNX Runtime to load WASM assets bundled by Vite with caching disabled
    const response = await fetch(wasmBinaryUrl, {
      cache: 'no-store',
      credentials: 'same-origin'
    });

    if (!response.ok) {
      throw new Error(`Failed to fetch ONNX Runtime wasm: ${response.status}`);
    }

    const wasmBinary = new Uint8Array(await response.arrayBuffer());

    ort.env.wasm.numThreads = 1;
    ort.env.wasm.simd = true;
    ort.env.wasm.wasmBinary = wasmBinary;
    ort.env.wasm.wasmPaths = {
      wasm: wasmBinaryUrl,
      mjs: wasmModuleUrl
    };

    // å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ç’°å¢ƒã«å¿œã˜ã¦è¨­å®š
    const providers = ['wasm'];

    // VADãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    try {
      console.log("ğŸ”„ VADãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...");
      this.vadSession = await ort.InferenceSession.create(
        '/models/silero-vad.onnx',
        { executionProviders: providers }
      );
      console.log("âœ… VADãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ");
    } catch (e) {
      console.error("âŒ VAD ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—:", e);
      throw new Error("VADãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ");
    }

    // åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    try {
      console.log("ğŸ”„ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...");
      this.embeddingSession = await ort.InferenceSession.create(
        '/models/wespeaker-simplified.onnx',
        { 
          executionProviders: providers,
          graphOptimizationLevel: 'all'
        }
      );
      console.log("âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ");
    } catch (e) {
      console.error("âŒ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—:", e);
      throw new Error("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ");
    }

    this.audioContext = new AudioContext({ sampleRate: 16000 });

    this.vadOutputName = this.resolveVadOutputName();
    this.vadStateOutputName = this.resolveVadStateOutputName();

    // é…åˆ—ã‚’åå‰ä»˜ãã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ï¼ˆã‚‚ã—é…åˆ—ã ã£ãŸå ´åˆï¼‰
    let vadInputMetadataRaw = this.vadSession.inputMetadata ?? {};
    let vadInputMetadata = Array.isArray(vadInputMetadataRaw)
      ? Object.fromEntries(vadInputMetadataRaw.map(entry => [entry.name, entry]))
      : vadInputMetadataRaw;

    // ğŸ”§ state ã®å…¥åŠ›åã‚’å¼·åˆ¶çš„ã«æŒ‡å®š
    const stateInputName = 'state';
    const stateOutputName = 'stateN';

    this.vadStateInputName = stateInputName;
    this.vadStateOutputName = stateOutputName;

    if (!vadInputMetadata[stateInputName]) {
      console.error("âŒ æŒ‡å®šã—ãŸ state å…¥åŠ›åãŒå­˜åœ¨ã—ã¾ã›ã‚“:", stateInputName);
      console.error("ğŸ“¥ vadInputMetadata:", vadInputMetadata);
      throw new Error(`VADãƒ¢ãƒ‡ãƒ«ã« ${stateInputName} ã¨ã„ã†å…¥åŠ›ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ`);
    }

    this.vadStateMeta = vadInputMetadata[stateInputName];
    this.resetVadState();

    const srMeta = vadInputMetadata.sr ?? null;
    if (srMeta) {
      this.vadSampleRateTensor = this.createConstantTensor(
        srMeta,
        this.audioContext.sampleRate
      );
    } else {
      this.vadSampleRateTensor = null;
    }
  }
  
  async diarizeAudio(audioBuffer) {
    if (this.vadStateMeta) {
      this.resetVadState();
    }

    // 1. éŸ³å£°åŒºé–“æ¤œå‡º
    const allSegments = await this.detectSpeech(audioBuffer);
    console.log(`${allSegments.length}å€‹ã®éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡º`);
    const speechSegments = this.filterShortSegments(allSegments, this.minOutputSegmentSec);
    if (speechSegments.length !== allSegments.length) {
      console.log(`çŸ­åŒºé–“ã‚’é™¤å¤–: ${allSegments.length - speechSegments.length}ä»¶ (< ${this.minOutputSegmentSec}s)`);
    }

    // 2. è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º
    let { embeddings, segmentMap } = await this.extractEmbeddings(speechSegments);
    ({ embeddings, segmentMap } = this.sanitizeEmbeddings(embeddings, segmentMap));

    if (embeddings.length === 0) {
      console.warn("âš ï¸ æœ‰åŠ¹ãªåŸ‹ã‚è¾¼ã¿ãŒå¾—ã‚‰ã‚Œãªã‹ã£ãŸãŸã‚ã€å˜ä¸€è©±è€…ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚");
      return speechSegments.map(segment => ({
        start: segment.start,
        end: segment.end,
        speaker: 'è©±è€…_0',
        confidence: 0
      }));
    }

    // 3. è©±è€…ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    const desired = this.fixedNumSpeakers ?? 2;
    const numSpeakers = Math.max(1, Math.min(desired, embeddings.length));
    let clusterResult;
    try {
      clusterResult = kmeans(embeddings, numSpeakers, {
        initialization: 'random', // k-means++ ã®è¡Œé¸æŠã‚¨ãƒ©ãƒ¼å›é¿
        maxIterations: 100
      });
    } catch (e) {
      console.warn("âš ï¸ KMeansã®åˆæœŸåŒ–ã«å¤±æ•—ã—ãŸãŸã‚ã€å˜ä¸€è©±è€…ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚", e);
      return speechSegments.map(segment => ({
        start: segment.start,
        end: segment.end,
        speaker: 'è©±è€…_0',
        confidence: 0
      }));
    }

    // 4. ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ï¼ˆåŸ‹ã‚è¾¼ã¿ãŒç”Ÿæˆã§ããŸåŒºé–“ã®ã¿å‡ºåŠ›ï¼‰
    const timeline = [];
    for (let embeddingIndex = 0; embeddingIndex < segmentMap.length; embeddingIndex++) {
      const segmentIndex = segmentMap[embeddingIndex];
      const segment = speechSegments[segmentIndex];
      const clusterId = clusterResult.clusters[embeddingIndex] ?? 0;
      const centroidEntry = clusterResult.centroids[clusterId];
      const centroid = Array.isArray(centroidEntry?.centroid)
        ? centroidEntry.centroid
        : centroidEntry;

      timeline.push({
        start: segment.start,
        end: segment.end,
        speaker: `è©±è€…_${clusterId}`,
        confidence: centroid
          ? this.calculateConfidence(embeddings[embeddingIndex], centroid)
          : 0
      });
    }
    return timeline;
  }
  filterShortSegments(segments, minSec = 0.3) {
    if (!Array.isArray(segments) || segments.length === 0) return [];
    return segments.filter(s => (s.end - s.start) >= minSec);
  }

  
  async detectSpeech(audioBuffer) {
    const segments = [];
    const windowSize = 512;
    const hopSize = 256;
    let inSpeech = false;
    let segmentStart = 0;

    for (let i = 0; i < audioBuffer.length - windowSize; i += hopSize) {
      const window = audioBuffer.slice(i, i + windowSize);
      const input = new ort.Tensor('float32', window, [1, windowSize]);

      const feeds = { input };

      // æ˜ç¤ºçš„ã« state ã‚’è¿½åŠ 
      if (!this.vadState) {
        throw new Error("VAD state ãŒæœªåˆæœŸåŒ–ã§ã™ã€‚initialize() ã§å¤±æ•—ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚");
      }
      feeds[this.vadStateInputName] = this.vadState;

      // sr ã‚‚ã‚ã‚Œã°è¿½åŠ 
      if (this.vadSampleRateTensor) {
        feeds.sr = this.vadSampleRateTensor;
      }

      const output = await this.vadSession.run(feeds);

      // å‡ºåŠ›åã®è§£æ±º
      const vadTensor = this.vadOutputName
        ? output[this.vadOutputName]
        : Object.values(output)[0];

      // state å‡ºåŠ›åã®è§£æ±ºï¼ˆå†ä»£å…¥ï¼‰
      if (this.vadStateOutputName && output[this.vadStateOutputName]) {
        this.vadState = output[this.vadStateOutputName];
      }

      const isSpeech = vadTensor.data[0] > 0.5;

      if (isSpeech && !inSpeech) {
        segmentStart = i;
        inSpeech = true;
      } else if (!isSpeech && inSpeech) {
        segments.push({
          start: segmentStart / this.audioContext.sampleRate,
          end: i / this.audioContext.sampleRate,
          audio: audioBuffer.slice(segmentStart, i)
        });
        inSpeech = false;
      }
    }

    if (inSpeech) {
      segments.push({
        start: segmentStart / this.audioContext.sampleRate,
        end: audioBuffer.length / this.audioContext.sampleRate,
        audio: audioBuffer.slice(segmentStart)
      });
    }

    return this.mergeShortSegments(segments, 0.3);
  }
  
  async extractEmbeddings(segments) {
    const embeddings = [];
    const segmentMap = [];

    for (let i = 0; i < segments.length; i++) {
      const segment = segments[i];
      const features = this.extractMFCC(segment.audio);
      if (features.length < 80) {
        console.warn(`âš ï¸ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ${i}ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚`);
        continue;
      }

      const usableLength = Math.floor(features.length / 80) * 80;
      const trimmed = features.slice(0, usableLength);

      const frames = usableLength / 80;
      const tensor = new ort.Tensor('float32', trimmed, [1, frames, 80]);

      const output = await this.embeddingSession.run({ feats: tensor });

      const embeddingTensor = output.embs;

      if (!embeddingTensor || !embeddingTensor.data) {
        console.warn("âš ï¸ åŸ‹ã‚è¾¼ã¿å‡ºåŠ›ãŒä¸æ­£ã¾ãŸã¯ç©ºã§ã™");
        continue;
      }

      // å®‰å…¨ãªãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆ[1,D] ã‹ [1,T,D] ã‚’æƒ³å®šï¼‰ã€‚æ™‚é–“æ–¹å‘ãŒã‚ã‚‹å ´åˆã¯å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã€‚
      const vector = this.poolEmbeddingTensor(embeddingTensor);
      if (!vector || !vector.length || !vector.every(Number.isFinite)) {
        console.warn("âš ï¸ åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒç„¡åŠ¹ï¼ˆNaN/Infinity/é•·ã•0ï¼‰ã ã£ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚");
        continue;
      }

      embeddings.push(this.l2Normalize(vector));
      segmentMap.push(i);
    }

    console.log("âœ… æŠ½å‡ºã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿æ•°:", embeddings.length);
    if (segmentMap.length !== segments.length) {
      console.warn(`â„¹ï¸ åŸ‹ã‚è¾¼ã¿ã«ä½¿ç”¨ã§ããªã‹ã£ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: ${segments.length - segmentMap.length}`);
    }

    return { embeddings, segmentMap };
  }

  l2Normalize(vec) {
    const norm = Math.hypot(...vec);
    if (!isFinite(norm) || norm === 0) return vec.slice();
    return vec.map(x => x / norm);
  }

  poolEmbeddingTensor(tensor) {
    const dims = Array.isArray(tensor.dims) ? tensor.dims : [];
    const data = tensor.data;
    if (!dims.length) return Array.from(data);

    // Expected shapes: [1, D] or [1, T, D]
    if (dims.length === 2) {
      const D = dims[1] ?? data.length;
      return Array.from(data).slice(0, D);
    }
    if (dims.length === 3) {
      const T = dims[1];
      const D = dims[2];
      const out = new Array(D).fill(0);
      for (let t = 0; t < T; t++) {
        const base = t * D;
        for (let d = 0; d < D; d++) out[d] += data[base + d];
      }
      for (let d = 0; d < D; d++) out[d] /= T || 1;
      return out;
    }
    // Fallback: treat the last dimension as a single vector
    const D = dims[dims.length - 1] || data.length;
    return Array.from(data).slice(-D);
  }

  sanitizeEmbeddings(embeddings, segmentMap) {
    if (!embeddings || embeddings.length === 0) return { embeddings: [], segmentMap: [] };

    // 1) Keep only finite-number vectors
    const finiteFiltered = [];
    const indexFiltered = [];
    for (let i = 0; i < embeddings.length; i++) {
      const v = embeddings[i];
      if (Array.isArray(v) && v.length && v.every(Number.isFinite)) {
        finiteFiltered.push(v);
        indexFiltered.push(segmentMap[i]);
      }
    }
    if (finiteFiltered.length === 0) return { embeddings: [], segmentMap: [] };

    // 2) Unify dimensionality: choose the most frequent length and truncate others
    const lengths = finiteFiltered.map(v => v.length);
    const freq = lengths.reduce((m, l) => (m.set(l, (m.get(l) || 0) + 1), m), new Map());
    let targetDim = lengths[0], maxFreq = 0;
    for (const [l, c] of freq.entries()) if (c > maxFreq) (maxFreq = c, targetDim = l);

    const sameDim = [];
    const sameDimIdx = [];
    for (let i = 0; i < finiteFiltered.length; i++) {
      const v = finiteFiltered[i];
      const vv = v.length === targetDim ? v : v.slice(0, targetDim);
      const norm = Math.hypot(...vv);
      if (norm > 0) {
        sameDim.push(vv);
        sameDimIdx.push(indexFiltered[i]);
      }
    }

    return { embeddings: sameDim, segmentMap: sameDimIdx };
  }
  
  extractMFCC(audioData) {
    // Meyda ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æŠ½å‡ºã€‚å¿…è¦ã«å¿œã˜ã¦ sampleRate / window ã‚’èª¿æ•´ã€‚
    const mfccs = [];
    const bufferSize = 512;
    const hopSize = 256;
    for (let i = 0; i + bufferSize <= audioData.length; i += hopSize) {
      const frame = audioData.slice(i, i + bufferSize);
      const features = Meyda.extract('mfcc', frame, { sampleRate: 16000, bufferSize });
      if (Array.isArray(features) && features.length) mfccs.push(...features);
    }
    return new Float32Array(mfccs);
  }

  estimateSpeakerCount(embeddings) {
    console.log("ğŸ§ª åŸ‹ã‚è¾¼ã¿æ•°:", embeddings.length);

    // ä¸€æ„ãƒ™ã‚¯ãƒˆãƒ«ã®æ¦‚ç®—ï¼ˆå°æ•°ç¬¬3ä½ã§ä¸¸ã‚ã¦æ¯”è¼ƒï¼‰
    const uniqueKeys = new Set(
      embeddings.map(v => v.map(x => Math.round(x * 1e3) / 1e3).join(','))
    );
    const uniqueCount = uniqueKeys.size;

    if (embeddings.length < 2 || uniqueCount < 2) {
      console.warn("â— ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ãã‚‹ã ã‘ã®å¤šæ§˜æ€§ãŒã‚ã‚Šã¾ã›ã‚“ã€‚");
      return 1;
    }

    const maxK = Math.min(10, uniqueCount, Math.floor(embeddings.length / 2));
    let bestK = 2;
    let bestError = Infinity;

    for (let k = 2; k <= maxK; k++) {
      if (embeddings.length < k) break;
      try {
        const result = kmeans(embeddings, k, { initialization: 'random', maxIterations: 50 });
        const err =
          (result.computeInformation && Number.isFinite(result.computeInformation.error))
            ? result.computeInformation.error
            : (Array.isArray(result.centroids)
              ? result.clusters.reduce((sum, cIdx, i) => {
                  const cEntry = result.centroids[cIdx];
                  const c = Array.isArray(cEntry?.centroid) ? cEntry.centroid : cEntry;
                  const v = embeddings[i];
                  const d2 = v.reduce((s, val, j) => {
                    const diff = val - c[j];
                    return s + diff * diff;
                  }, 0);
                  return sum + d2;
                }, 0)
              : Infinity);

        if (err < bestError * 0.9) { // æ˜ç¢ºãªæ”¹å–„ãŒã‚ã‚‹å ´åˆã®ã¿æ›´æ–°
          bestError = err;
          bestK = k;
        }
      } catch (e) {
        console.warn(`k=${k} ã®kmeansã§å¤±æ•—:`, e);
        break;
      }
    }

    console.log("ğŸ§© å¾—ã‚‰ã‚ŒãŸåŸ‹ã‚è¾¼ã¿æ•°:", embeddings.length, "unique:", uniqueCount, "bestK:", bestK);
    return Math.min(bestK, uniqueCount, embeddings.length);
  }
  
  calculateConfidence(embedding, centroid) {
    // ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã¨ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ä½¿ç”¨
    const dotProduct = embedding.reduce((sum, val, i) => 
      sum + val * centroid[i], 0);
    const normA = Math.sqrt(embedding.reduce((sum, val) => 
      sum + val * val, 0));
    const normB = Math.sqrt(centroid.reduce((sum, val) => 
      sum + val * val, 0));
    
    return dotProduct / (normA * normB);
  }

  resetVadState() {
    if (!this.vadStateMeta) {
      this.vadState = null;
      return;
    }
    
    this.vadState = this.createConstantTensor(this.vadStateMeta, 0);
  }

  resolveVadOutputName() {
    if (!this.vadSession) return null;
    if (this.vadSession.outputNames.includes('output')) {
      return 'output';
    }
    return this.vadSession.outputNames[0] ?? null;
  }

  resolveVadStateOutputName() {
    if (!this.vadSession) return null;
    const candidates = this.vadSession.outputNames.filter(
      name => name !== this.vadOutputName
    );
    const stateName = candidates.find(name => name.toLowerCase().includes('state'));
    return stateName ?? candidates[0] ?? null;
  }

  createConstantTensor(meta, value) {
    const shape = this.getConcreteShape(meta?.dimensions ?? meta?.shape);
    const size = shape.reduce((total, dim) => total * dim, 1);
    const type = this.normalizeTensorType(meta?.type);
    const typedArray = this.createTypedArray(type, size);
    
    if (type === 'bool') {
      typedArray.fill(value ? 1 : 0);
    } else if (type === 'int64') {
      typedArray.fill(BigInt(value));
    } else {
      typedArray.fill(value);
    }
    
    return new ort.Tensor(type, typedArray, shape);
  }

  getConcreteShape(shape) {
    if (!shape || shape.length === 0) {
      return [1];
    }

    return shape.map(dim => {
      if (typeof dim === 'number') {
        // ä¸å®šå€¤ï¼ˆ-1ï¼‰ã‚„ç•°å¸¸ãªå¤§ãã•ã‚’é¿ã‘ã‚‹
        return dim > 0 && dim < 10000 ? dim : 1;
      }
      // æ–‡å­—åˆ—ï¼ˆ'?'ï¼‰ã‚„ undefined å¯¾ç­–
      return 1;
    });
  }

  createTypedArray(type, size) {
    switch (type) {
      case 'float32':
        return new Float32Array(size);
      case 'float64':
        return new Float64Array(size);
      case 'int32':
        return new Int32Array(size);
      case 'int16':
        return new Int16Array(size);
      case 'int8':
        return new Int8Array(size);
      case 'uint32':
        return new Uint32Array(size);
      case 'uint16':
        return new Uint16Array(size);
      case 'uint8':
      case 'bool':
        return new Uint8Array(size);
      case 'int64':
        return new BigInt64Array(size);
      default:
        throw new Error(`Unsupported tensor type: ${type}`);
    }
  }

  normalizeTensorType(type) {
    if (!type) {
      return 'float32';
    }
    if (type.startsWith('tensor(') && type.endsWith(')')) {
      return type.slice(7, -1);
    }
    return type;
  }
  
  mergeShortSegments(segments, minGap = 0.3) {
    if (segments.length === 0) return [];
    
    const merged = [segments[0]];
    
    for (let i = 1; i < segments.length; i++) {
      const lastSegment = merged[merged.length - 1];
      
      if (segments[i].start - lastSegment.end < minGap) {
        // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ¼ã‚¸
        lastSegment.end = segments[i].end;
        lastSegment.audio = new Float32Array([
          ...lastSegment.audio,
          ...segments[i].audio
        ]);
      } else {
        merged.push(segments[i]);
      }
    }
    
    return merged;
  }
}

// ä½¿ç”¨ä¾‹
async function processMeetingAudio() {
  const diarizer = new BrowserSpeakerDiarization();
  await diarizer.initialize();
  
  // ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›ã‹ã‚‰éŸ³å£°ã‚’å–å¾—
  const fileInput = document.getElementById('audioFile');
  const audioFile = fileInput.files[0];
  const arrayBuffer = await audioFile.arrayBuffer();
  
  // éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
  const audioBuffer = await diarizer.audioContext.decodeAudioData(arrayBuffer);
  const audioData = audioBuffer.getChannelData(0); // ãƒ¢ãƒãƒãƒ£ãƒ³ãƒãƒ«ã‚’å–å¾—
  
  // è©±è€…åˆ†é›¢ã‚’å®Ÿè¡Œ
  const timeline = await diarizer.diarizeAudio(audioData);
  
  // çµæœã‚’è¡¨ç¤º
  timeline.forEach(segment => {
    console.log(
      `${segment.speaker}: ${segment.start.toFixed(2)}ç§’ - ${segment.end.toFixed(2)}ç§’ ` +
      `(ä¿¡é ¼åº¦: ${(segment.confidence * 100).toFixed(1)}%)`
    );
  });
  
  return timeline;
}

export { BrowserSpeakerDiarization };
